{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word_embedding.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeAzFVIQAGZl",
        "colab_type": "text"
      },
      "source": [
        "# Word Embedding: A quick intro"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q31YtUx--Nik",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "64cf6586-087d-4ea0-ed9e-a62df345f90f"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.0-rc4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J40mBl-tDOjW",
        "colab_type": "text"
      },
      "source": [
        "## Download Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0hbdNDRAMz_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPjj-x5GDRvV",
        "colab_type": "text"
      },
      "source": [
        "## Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvRM8oUM_5zX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "train_data, test_data = imdb['train'], imdb['test']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEVCv6HIBU-B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "18de485f-a4f7-44a2-e93a-f139dad9d085"
      },
      "source": [
        "train_sentences=[]\n",
        "train_labels=[]\n",
        "test_sentences=[]\n",
        "test_labels=[]\n",
        "\n",
        "for sentence, label in train_data:\n",
        "  train_sentences.append(str(sentence.numpy()))\n",
        "  train_labels.append(label.numpy())\n",
        "\n",
        "for sentence, label in test_data:\n",
        "  test_sentences.append(str(sentence.numpy()))\n",
        "  test_labels.append(label.numpy())\n",
        "\n",
        "#Example\n",
        "print(train_labels[0])\n",
        "print(train_sentences[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rJ-XXqNBxsI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = np.array(train_labels)\n",
        "test_labels = np.array(test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8F8Ds4FDeFq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE = 10000\n",
        "EMBEDDING_DIM = 16\n",
        "MAX_LENGTH = 120\n",
        "TRUNC_TYPE = 'post'\n",
        "OOV_TOKEN = '<OOV>'\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=OOV_TOKEN)\n",
        "tokenizer.fit_on_texts(train_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "padded = pad_sequences(sequences=sequences, maxlen=MAX_LENGTH)\n",
        "\n",
        "test_sequencies = tokenizer.texts_to_sequences(test_sentences)\n",
        "test_padded = pad_sequences(sequences=test_sequencies, maxlen=MAX_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbbHt_0yE9Kn",
        "colab_type": "text"
      },
      "source": [
        "## Fit the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVUSjEnNJpmU",
        "colab_type": "text"
      },
      "source": [
        "### Model A: Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sz3LYiO0E_Jp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "01f4e41e-ee7d-4317-b829-f1b5bfdfb265"
      },
      "source": [
        "model_a = tf.keras.Sequential([\n",
        "                             tf.keras.layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH),\n",
        "                             tf.keras.layers.Flatten(), \n",
        "                            #  tf.keras.layers.GlobalAveragePooling1D(),  #Used with embedding. It averages across the vector to flatten it out. \n",
        "                             tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_a.summary()\n",
        "\n",
        "model_a.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "model_a.fit(x=padded, y=train_labels, epochs=10, verbose=2, validation_data=(test_padded, test_labels))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 120, 16)           160000    \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1920)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 1921      \n",
            "=================================================================\n",
            "Total params: 161,921\n",
            "Trainable params: 161,921\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "782/782 - 3s - loss: 0.4987 - accuracy: 0.7562 - val_loss: 0.3356 - val_accuracy: 0.8564\n",
            "Epoch 2/10\n",
            "782/782 - 3s - loss: 0.2624 - accuracy: 0.8958 - val_loss: 0.3193 - val_accuracy: 0.8603\n",
            "Epoch 3/10\n",
            "782/782 - 3s - loss: 0.1809 - accuracy: 0.9371 - val_loss: 0.3230 - val_accuracy: 0.8591\n",
            "Epoch 4/10\n",
            "782/782 - 3s - loss: 0.1172 - accuracy: 0.9678 - val_loss: 0.3586 - val_accuracy: 0.8506\n",
            "Epoch 5/10\n",
            "782/782 - 3s - loss: 0.0692 - accuracy: 0.9874 - val_loss: 0.3764 - val_accuracy: 0.8502\n",
            "Epoch 6/10\n",
            "782/782 - 3s - loss: 0.0377 - accuracy: 0.9964 - val_loss: 0.4200 - val_accuracy: 0.8453\n",
            "Epoch 7/10\n",
            "782/782 - 3s - loss: 0.0205 - accuracy: 0.9991 - val_loss: 0.4401 - val_accuracy: 0.8468\n",
            "Epoch 8/10\n",
            "782/782 - 3s - loss: 0.0116 - accuracy: 0.9998 - val_loss: 0.4704 - val_accuracy: 0.8461\n",
            "Epoch 9/10\n",
            "782/782 - 3s - loss: 0.0066 - accuracy: 0.9999 - val_loss: 0.5060 - val_accuracy: 0.8458\n",
            "Epoch 10/10\n",
            "782/782 - 3s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.5329 - val_accuracy: 0.8448\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8d3753d898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nVKqGarPSYb",
        "colab_type": "text"
      },
      "source": [
        "#### Comment \n",
        "\n",
        "Overfitting!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-JgzdJdJvwB",
        "colab_type": "text"
      },
      "source": [
        "### Model B: Global Average"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5z1LHO6Ju7q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "4377b9a7-3668-4dc4-c22c-0186cf73c262"
      },
      "source": [
        "model_b = tf.keras.Sequential([\n",
        "                             tf.keras.layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH),\n",
        "                            #  tf.keras.layers.Flatten(), \n",
        "                             tf.keras.layers.GlobalAveragePooling1D(),  #Used with embedding. It averages across the vector to flatten it out. \n",
        "                             tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_b.summary()\n",
        "\n",
        "model_b.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "model_b.fit(x=padded, y=train_labels, epochs=10, verbose=2, validation_data=(test_padded, test_labels))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 120, 16)           160000    \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d (Gl (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 160,017\n",
            "Trainable params: 160,017\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "782/782 - 3s - loss: 0.6223 - accuracy: 0.7402 - val_loss: 0.5284 - val_accuracy: 0.7999\n",
            "Epoch 2/10\n",
            "782/782 - 3s - loss: 0.4451 - accuracy: 0.8378 - val_loss: 0.4076 - val_accuracy: 0.8365\n",
            "Epoch 3/10\n",
            "782/782 - 3s - loss: 0.3534 - accuracy: 0.8650 - val_loss: 0.3591 - val_accuracy: 0.8498\n",
            "Epoch 4/10\n",
            "782/782 - 3s - loss: 0.3068 - accuracy: 0.8799 - val_loss: 0.3351 - val_accuracy: 0.8581\n",
            "Epoch 5/10\n",
            "782/782 - 3s - loss: 0.2763 - accuracy: 0.8923 - val_loss: 0.3237 - val_accuracy: 0.8606\n",
            "Epoch 6/10\n",
            "782/782 - 3s - loss: 0.2535 - accuracy: 0.9010 - val_loss: 0.3183 - val_accuracy: 0.8612\n",
            "Epoch 7/10\n",
            "782/782 - 3s - loss: 0.2356 - accuracy: 0.9083 - val_loss: 0.3187 - val_accuracy: 0.8618\n",
            "Epoch 8/10\n",
            "782/782 - 3s - loss: 0.2201 - accuracy: 0.9151 - val_loss: 0.3184 - val_accuracy: 0.8616\n",
            "Epoch 9/10\n",
            "782/782 - 3s - loss: 0.2068 - accuracy: 0.9217 - val_loss: 0.3211 - val_accuracy: 0.8607\n",
            "Epoch 10/10\n",
            "782/782 - 3s - loss: 0.1949 - accuracy: 0.9274 - val_loss: 0.3274 - val_accuracy: 0.8589\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8d356e7208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Agy8fe--PaDM",
        "colab_type": "text"
      },
      "source": [
        "## Visualize Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZalRkb0MPd1x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a44ad07e-5483-4130-dbdf-fbbbb01ee8db"
      },
      "source": [
        "e = model_b.layers[0]\n",
        "weights = e.get_weights()[0]\n",
        "print('(vocab_size, embedding_data)')\n",
        "print(weights.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(vocab_size, embedding_data)\n",
            "(10000, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGY4E_YrPoUH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reverse_word_index = dict([(v,k) for (k,v) in word_index.items()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa9aZMVVQdG9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now it's time to write the vectors and their metadata auto files. \n",
        "# The TensorFlow Projector reads this file type and uses it to plot \n",
        "#the vectors in 3D space so we can visualize them.\n",
        "\n",
        "import io\n",
        "\n",
        "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
        "for word_num in range(1, VOCAB_SIZE):\n",
        "  word = reverse_word_index[word_num]\n",
        "  embeddings = weights[word_num]\n",
        "  out_m.write(word + \"\\n\")\n",
        "  out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pipy0kY7Q1H8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  from google.colab import files\n",
        "except ImportError:\n",
        "  pass\n",
        "else:\n",
        "  files.download('vecs.tsv')\n",
        "  files.download('meta.tsv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6gcaS7YS-fo",
        "colab_type": "text"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHwJUf-9S_th",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "11b45242-89d1-47fe-9447-3954891d6c5b"
      },
      "source": [
        "sentence = \"I really think this is amazing. honest.\"\n",
        "sequence = tokenizer.texts_to_sequences(sentence)\n",
        "print(sequence)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[11], [], [1431], [966], [4], [1537], [1537], [4715], [], [790], [2019], [11], [2929], [2184], [], [790], [2019], [11], [579], [], [11], [579], [], [4], [1782], [4], [4517], [11], [2929], [1275], [], [], [2019], [1003], [2929], [966], [579], [790], []]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}